{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T15:51:12.570522Z",
     "start_time": "2019-10-29T15:51:12.564840Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "7LOWWuAwQXAC",
    "outputId": "5f85d235-4c90-4a70-a3b8-2d350535db40",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "# plt.style.use('ggplot')\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, Conv1D, Flatten, MaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "filterwarnings('ignore')\n",
    "# % matplotlib\n",
    "# inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T15:47:10.574444Z",
     "start_time": "2019-10-29T15:47:10.560877Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "TxHeO3NXQXAF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set data folder path\n",
    "DATA_FOLDER = r'./MAREA_dataset'\n",
    "ACTIVITY_FOLDER = os.path.join(DATA_FOLDER, 'Activity Timings')\n",
    "SUBJECT_FOLDER = os.path.join(DATA_FOLDER, 'Subject Data_txt format')\n",
    "PROCESSED_FOLDER = os.path.join(DATA_FOLDER, 'Processed_data')\n",
    "\n",
    "# define activity timing labels\n",
    "label_indoor = ['tread_flat_walk_start',\n",
    "                'tread_flat_walk_end',\n",
    "                'tread_flat_run_end',\n",
    "                'tread_slope_walk_start',\n",
    "                'tread_slope_walk_end',\n",
    "                'indoor_flat_walk_start',\n",
    "                'indoor_flat_walk_end',\n",
    "                'indoor_flat_run_end']\n",
    "\n",
    "label_outdoor = ['outdoor_walk_start',\n",
    "                 'outdoor_walk_end',\n",
    "                 'outdoor_run_end']\n",
    "\n",
    "# prepare timing index for different activities\n",
    "df_indoor_time = pd.read_csv(os.path.join(ACTIVITY_FOLDER, 'Indoor Experiment Timings.txt')\n",
    "                             , names=label_indoor)\n",
    "\n",
    "df_outdoor_time = pd.read_csv(os.path.join(ACTIVITY_FOLDER, 'Outdoor Experiment Timings.txt')\n",
    "                              , names=label_outdoor)\n",
    "\n",
    "df_indoor_time[\"subject\"] = [\"Sub\" + str(i) for i in range(1, 12)]\n",
    "df_outdoor_time[\"subject\"] = [\"Sub\" + str(j) for j in range(12, 21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T15:47:10.605642Z",
     "start_time": "2019-10-29T15:47:10.600997Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "mANgNCveQXAH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set up activity column names\n",
    "axis_list = ['accX', 'accY', 'accZ']\n",
    "pos_list = ['LF', 'RF', 'Waist', 'Wrist']\n",
    "sub_list = ['Sub' + str(i) for i in range(1, 21)]\n",
    "column_names = [f\"{y}_{x}\" for x, y in itertools.product(pos_list, axis_list)]\n",
    "\n",
    "# TODO: purposely exclude subject 4 first as missing data -- dont know how to deal with missing data for signal\n",
    "sub_list.remove('Sub4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T15:47:10.641298Z",
     "start_time": "2019-10-29T15:47:10.635528Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8mIdUglqQXAJ",
    "outputId": "c096e594-347e-4b5b-9630-0e54470b23f4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T15:47:20.602788Z",
     "start_time": "2019-10-29T15:47:10.666629Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "txX51RJXQXAL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create master dataframe\n",
    "const_master_df = pd.DataFrame()\n",
    "for sub in sub_list:\n",
    "    df_lf = pd.read_csv(os.path.join(SUBJECT_FOLDER, sub + '_' + 'LF.txt'))\n",
    "    df_rf = pd.read_csv(os.path.join(SUBJECT_FOLDER, sub + '_' + 'RF.txt'))\n",
    "    df_waist = pd.read_csv(os.path.join(SUBJECT_FOLDER, sub + '_' + 'Waist.txt'))\n",
    "    df_wrist = pd.read_csv(os.path.join(SUBJECT_FOLDER, sub + '_' + 'Wrist.txt'))\n",
    "    df_sub = pd.concat([df_lf, df_rf, df_waist, df_wrist], axis=1)\n",
    "    df_sub.columns = column_names\n",
    "\n",
    "    df_sub = df_sub.copy()\n",
    "    n = int(sub[3:])\n",
    "    if n > 11:\n",
    "        sub_row = df_outdoor_time[df_outdoor_time['subject'] == sub]\n",
    "        tmp = sub_row.iloc[0]\n",
    "        df_sub.loc[0:tmp['outdoor_walk_end'], 'label'] = 'outdoor_walk'\n",
    "        df_sub.loc[tmp['outdoor_walk_end']: tmp['outdoor_run_end'], 'label'] = 'outdoor_run'\n",
    "    else:\n",
    "        sub_row = df_indoor_time[df_indoor_time['subject'] == sub]\n",
    "        tmp = sub_row.iloc[0]\n",
    "        df_sub.loc[0:tmp['tread_flat_walk_end'], 'label'] = 'tread_flat_walk'\n",
    "        df_sub.loc[tmp['tread_flat_walk_end']: tmp['tread_flat_run_end'], 'label'] = 'tread_flat_run'\n",
    "        df_sub.loc[tmp['tread_flat_run_end']: tmp['tread_slope_walk_start'], 'label'] = 'rest'\n",
    "        df_sub.loc[tmp['tread_slope_walk_start']: tmp['tread_slope_walk_end'], 'label'] = 'tread_slope_walk'\n",
    "        df_sub.loc[tmp['tread_slope_walk_end']: tmp['indoor_flat_walk_start'], 'label'] = 'rest'\n",
    "        df_sub.loc[tmp['indoor_flat_walk_start']: tmp['indoor_flat_walk_end'], 'label'] = 'indoor_flat_walk'\n",
    "        df_sub.loc[tmp['indoor_flat_walk_end']: tmp['indoor_flat_run_end'], 'label'] = 'indoor_flat_run'\n",
    "\n",
    "    df_sub['subject'] = sub\n",
    "    const_master_df = const_master_df.append(df_sub)\n",
    "    # print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T15:47:20.710985Z",
     "start_time": "2019-10-29T15:47:20.694541Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xBZMZuXAQXAN",
    "outputId": "946176e4-f72d-40e0-8e91-7a1eb6369f46",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "const_master_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T15:47:20.801969Z",
     "start_time": "2019-10-29T15:47:20.791894Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "n_mjZbIPQXAP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def PreprocessingSignal(df, label, subject, feature, window,\n",
    "                        wavelet_args={\"type\": \"Y\",\n",
    "                                      \"threshold\": 2,\n",
    "                                      \"wavedec_options\": {\"wavelet\": \"db4\", \"level\": 2},\n",
    "                                      \"waverec_options\": {\"wavelet\": \"db4\"}},\n",
    "                        window_args={\"type\": \"no_overlap\"}\n",
    "                        ):\n",
    "    df = df.loc[(df['label'] == label) & (df['subject'] == subject)]\n",
    "\n",
    "    ### Do wavelet transform or NOT ###\n",
    "    if wavelet_args[\"type\"] == \"Y\":\n",
    "        # wavelet_args = {\"threshold\":2, \"options\":{\"wavelet\":\"db4\", \"level\":0.8}}\n",
    "        # Do wavelet transform\n",
    "        signal_orig = df[feature].values\n",
    "        args1 = wavelet_args[\"wavedec_options\"]\n",
    "        coeffs_orig = pywt.wavedec(signal_orig, **args1)\n",
    "        coeffs_filter = coeffs_orig.copy()\n",
    "        threshold = wavelet_args[\"threshold\"]\n",
    "        for i in range(1, len(coeffs_orig)):\n",
    "            coeffs_filter[i] = pywt.threshold(coeffs_orig[i], threshold * max(coeffs_orig[i]))\n",
    "        args2 = wavelet_args[\"waverec_options\"]\n",
    "        signal_denoised = pywt.waverec(coeffs_filter, **args2)\n",
    "        to_process_df = pd.DataFrame(signal_denoised)\n",
    "    else:\n",
    "        tmp_df = df[feature].reset_index()\n",
    "        to_process_df = tmp_df.drop(columns=[\"index\"])\n",
    "        to_process_df.columns = [0]\n",
    "    ### Do wavelet transform or NOT ###\n",
    "\n",
    "    min_index = min(to_process_df.index)\n",
    "    max_index = max(to_process_df.index)\n",
    "\n",
    "    ### Define Method to cut signal into windows ###\n",
    "    if window_args[\"type\"] == \"no_overlap\":\n",
    "        # window_args = {\"type\":\"no_overlap\"}\n",
    "        index_list = range(min_index, max_index + 1, int(window))\n",
    "    elif window_args[\"type\"] == \"with_overlap\":\n",
    "        # window_args = {\"type\":\"with_overlap\", \"overlap_perc\":0.5}\n",
    "        overlap_perc = window_args[\"overlap_perc\"]\n",
    "        step = int(window / (1 / overlap_perc))\n",
    "        index_list = range(min_index, max_index + 1, step)\n",
    "    elif window_args[\"type\"] == \"by_peaks\":\n",
    "        index_list = window_args[\"peaks_index\"]\n",
    "    ### Define Method to cut signal into windows ###\n",
    "\n",
    "    ### Cut signal into windows ###\n",
    "    windowed_selected_chunk_array = []\n",
    "    for index in index_list:\n",
    "        windowed_selected_chunk = to_process_df[0].iloc[index:index + window]\n",
    "        if windowed_selected_chunk.shape[0] == window:\n",
    "            windowed_selected_chunk_array.append(windowed_selected_chunk.values)\n",
    "    output_np_arr = np.array(windowed_selected_chunk_array)\n",
    "    ### Cut signal into windows ###\n",
    "\n",
    "    output_np_label = np.asarray([label] * output_np_arr.shape[0])\n",
    "\n",
    "    return output_np_arr, output_np_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T15:47:20.897198Z",
     "start_time": "2019-10-29T15:47:20.893393Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "W_NwdUigQXAR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# feature_list = column_names TODO: is this line still needed?\n",
    "\n",
    "# Create\n",
    "const_indoor_activity = ['rest',\n",
    "                         'tread_flat_walk',\n",
    "                         'tread_flat_run',\n",
    "                         'tread_slope_walk',\n",
    "                         'indoor_flat_walk',\n",
    "                         'indoor_flat_run'\n",
    "                         ]\n",
    "const_outdoor_activity = ['outdoor_walk', 'outdoor_run']\n",
    "\n",
    "const_indoor_sub = ['Sub' + str(i) for i in range(1, 12)]\n",
    "const_outdoor_sub = ['Sub' + str(i) for i in range(12, 21)]\n",
    "const_indoor_sub.remove('Sub4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T15:47:21.012636Z",
     "start_time": "2019-10-29T15:47:20.977016Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "IIH_KN_8QXAS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def PrepareFeature(df, feature, feature_col, split_method, window, wavelet_args, window_args, train_size=0.8):\n",
    "    X_train = pd.DataFrame()\n",
    "    X_test = pd.DataFrame()\n",
    "    y_train = pd.DataFrame()\n",
    "    y_test = pd.DataFrame()\n",
    "\n",
    "    # TODO: find_peaks from a given column\n",
    "    # NOTE: code will break when using -> window_args = {\"type\":\"by_peaks\", \"peaks_index\":peaks_index_list}\n",
    "\n",
    "    # method 1: split within each subject\n",
    "    if split_method == 'TrainTestSplitWithinSubject':\n",
    "        # indoor activity\n",
    "        for sub in const_indoor_sub:\n",
    "            for activity in const_indoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature_col\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "\n",
    "                output_train_len = int(train_size * len(output_np_arr))\n",
    "                tr_x = output_np_arr[:output_train_len, :]\n",
    "                ts_x = output_np_arr[output_train_len:len(output_np_arr), :]\n",
    "\n",
    "                label_train_len = int(train_size * len(output_label))\n",
    "                tr_y = output_label[:label_train_len]\n",
    "                ts_y = output_label[label_train_len:len(output_label)]\n",
    "\n",
    "                X_train = X_train.append(pd.DataFrame(tr_x))\n",
    "                X_test = X_test.append(pd.DataFrame(ts_x))\n",
    "                y_train = y_train.append(pd.DataFrame(tr_y))\n",
    "                y_test = y_test.append(pd.DataFrame(ts_y))\n",
    "\n",
    "                # outdoor activity\n",
    "        for sub in const_outdoor_sub:\n",
    "            for activity in const_outdoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature_col\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "\n",
    "                output_train_len = int(train_size * len(output_np_arr))\n",
    "                tr_x = output_np_arr[:output_train_len, :]\n",
    "                ts_x = output_np_arr[output_train_len:len(output_np_arr), :]\n",
    "\n",
    "                label_train_len = int(train_size * len(output_label))\n",
    "                tr_y = output_label[:label_train_len]\n",
    "                ts_y = output_label[label_train_len:len(output_label)]\n",
    "\n",
    "                X_train = X_train.append(pd.DataFrame(tr_x))\n",
    "                X_test = X_test.append(pd.DataFrame(ts_x))\n",
    "                y_train = y_train.append(pd.DataFrame(tr_y))\n",
    "                y_test = y_test.append(pd.DataFrame(ts_y))\n",
    "\n",
    "    # method 2: split all samples randomly\n",
    "    if split_method == 'Random':\n",
    "        #             output = np.asarray(list())\n",
    "        #             label = np.asarray(list())\n",
    "        output = np.empty((0, window), float)\n",
    "        label = np.empty((0,), float)\n",
    "\n",
    "        # indoor activity\n",
    "        for sub in const_indoor_sub:\n",
    "            for activity in const_indoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature_col\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "                output = np.append(output, output_np_arr, axis=0)\n",
    "                label = np.append(label, output_label, axis=0)\n",
    "\n",
    "        # outdoor activity:\n",
    "        for sub in const_outdoor_sub:\n",
    "            for activity in const_outdoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature_col\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "                output = np.append(output, output_np_arr, axis=0)\n",
    "                label = np.append(label, output_label, axis=0)\n",
    "\n",
    "    \n",
    "        shuffle_index = np.arange(len(label))\n",
    "        np.random.RandomState(42).shuffle(shuffle_index)\n",
    "\n",
    "        output_train_len = int(train_size * len(output))\n",
    "        tr_x = output[shuffle_index][:output_train_len, :]\n",
    "        ts_x = output[shuffle_index][output_train_len:len(output), :]\n",
    "\n",
    "        label_train_len = int(train_size * len(label))\n",
    "        tr_y = label[shuffle_index][:label_train_len]\n",
    "        ts_y = label[shuffle_index][label_train_len:len(label)]\n",
    "\n",
    "        X_train = X_train.append(pd.DataFrame(tr_x))\n",
    "        X_test = X_test.append(pd.DataFrame(ts_x))\n",
    "        y_train = y_train.append(pd.DataFrame(tr_y))\n",
    "        y_test = y_test.append(pd.DataFrame(ts_y))\n",
    "\n",
    "    # method 3: only keep several subjects in the train,\n",
    "    #           put other subjects in the test\n",
    "    if split_method == 'DifferentSubjectsInTrainTest':\n",
    "        train_sub = np.empty((0, window), float)\n",
    "        train_label_sub = np.empty((0,), float)\n",
    "        test_sub = np.empty((0, window), float)\n",
    "        test_label_sub = np.empty((0,), float)\n",
    "\n",
    "        # indoor activity\n",
    "        indoor_sub_train_len = int(train_size * len(const_indoor_sub))\n",
    "        indoor_sub_train = const_indoor_sub[:indoor_sub_train_len]\n",
    "        indoor_sub_test = const_indoor_sub[indoor_sub_train_len:len(const_indoor_sub)]\n",
    "\n",
    "        for sub in indoor_sub_train:\n",
    "            for activity in const_indoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature_col\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    # print(\"THIS\", activity, peaks_index_list)\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "                train_sub = np.append(train_sub, output_np_arr, axis=0)\n",
    "                train_label_sub = np.append(train_label_sub, output_label, axis=0)\n",
    "\n",
    "        for sub in indoor_sub_test:\n",
    "            for activity in const_indoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature_col\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "                test_sub = np.append(test_sub, output_np_arr, axis=0)\n",
    "                test_label_sub = np.append(test_label_sub, output_label, axis=0)\n",
    "\n",
    "        # outdoor activity\n",
    "        outdoor_sub_train_len = int(train_size * len(const_outdoor_sub))\n",
    "        outdoor_sub_train = const_outdoor_sub[:outdoor_sub_train_len]\n",
    "        outdoor_sub_test = const_outdoor_sub[outdoor_sub_train_len:len(const_outdoor_sub)]\n",
    "\n",
    "        for sub in outdoor_sub_train:\n",
    "            for activity in const_outdoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature_col\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "                train_sub = np.append(train_sub, output_np_arr, axis=0)\n",
    "                train_label_sub = np.append(train_label_sub, output_label, axis=0)\n",
    "\n",
    "        for sub in outdoor_sub_test:\n",
    "            for activity in const_outdoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature_col\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "                test_sub = np.append(test_sub, output_np_arr, axis=0)\n",
    "                test_label_sub = np.append(test_label_sub, output_label, axis=0)\n",
    "\n",
    "        X_train = pd.DataFrame(train_sub)\n",
    "        y_train = pd.DataFrame(train_label_sub)\n",
    "        X_test = pd.DataFrame(test_sub)\n",
    "        y_test = pd.DataFrame(test_label_sub)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T15:47:21.104757Z",
     "start_time": "2019-10-29T15:47:21.094668Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "RNWPaMgOQXAU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SAMPLE #\n",
    "\n",
    "def prepare_single_feature_to_csv(feature='accY_LF', feature_col='accY_LF', model_name='v1', split_method='TrainTestSplitWithinSubject', \n",
    "                                  window=512, wavelet_args={\"type\": \"N\"},\n",
    "                                  window_args={\"type\": \"by_peaks\", \"find_peaks_col\": \"accX_LF\",\n",
    "                                               \"find_peaks_options\": {\"prominence\": 30, \"height\": 20}}):\n",
    "    # feature = 'accY_LF'\n",
    "    # feature_list = column_names\n",
    "    # for feature in column_names:\n",
    "    # (example) feature = accX_LF (which is stored in the column_names)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = PrepareFeature(const_master_df, feature, feature_col, split_method, window, wavelet_args,\n",
    "                                                      window_args)\n",
    "\n",
    "    X_train_filename = \"_\".join(\n",
    "        ['X_train', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "         split_method]) + '.txt'\n",
    "    X_test_filename = \"_\".join(\n",
    "        ['X_test', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "         split_method]) + '.txt'\n",
    "    y_train_filename = \"_\".join(\n",
    "        ['y_train', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "         split_method]) + '.txt'\n",
    "    y_test_filename = \"_\".join(\n",
    "        ['y_test', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "         split_method]) + '.txt'\n",
    "\n",
    "    WORK_FOLDER = r'./Models'\n",
    "    MODEL_FOLDER = os.path.join(WORK_FOLDER, model_name) \n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        os.makedirs(MODEL_FOLDER)\n",
    "    \n",
    "    X_train.to_csv(os.path.join(MODEL_FOLDER, X_train_filename), header=None, index=False, sep='\\t')\n",
    "    X_test.to_csv(os.path.join(MODEL_FOLDER, X_test_filename), header=None, index=False, sep='\\t')\n",
    "\n",
    "    pd.DataFrame(y_train).to_csv(os.path.join(MODEL_FOLDER,y_train_filename), header=None, index=False, sep='\\t')\n",
    "    pd.DataFrame(y_test).to_csv(os.path.join(MODEL_FOLDER,y_test_filename), header=None, index=False, sep='\\t')\n",
    "\n",
    "    print(\"Saved X_train to %s\" % (X_train_filename))\n",
    "    print(\"Saved X_test to %s\" % (X_test_filename))\n",
    "    print(\"Saved X_train to %s\" % (y_train_filename))\n",
    "    print(\"Saved X_test to %s\" % (y_test_filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T15:47:21.188701Z",
     "start_time": "2019-10-29T15:47:21.185857Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3D7dNuLWQXAW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_file(model_folder, filename):\n",
    "    \n",
    "    filepath = os.path.join(model_folder, filename)\n",
    "  \n",
    "    dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T15:47:21.273800Z",
     "start_time": "2019-10-29T15:47:21.269451Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Ma19gRBmQXAY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_group(model_folder, filenames):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(model_folder, name)\n",
    "        loaded.append(data)\n",
    "\n",
    "    loaded = np.dstack(loaded)\n",
    "    return loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T15:47:21.369111Z",
     "start_time": "2019-10-29T15:47:21.360164Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OlPcsJN5QXAZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(model_name, feature_list, window, wavelet_args, window_args, split_method):\n",
    "\n",
    "    WORK_FOLDER = r'./Models'\n",
    "    MODEL_FOLDER = os.path.join(WORK_FOLDER, model_name) \n",
    "    \n",
    "    X_train_filenames = []\n",
    "    X_test_filenames = []\n",
    "\n",
    "    for feature in feature_list:\n",
    "        X_train_filenames.append(\"_\".join(\n",
    "            ['X_train', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "             split_method]) + '.txt')\n",
    "\n",
    "        X_test_filenames.append(\"_\".join(\n",
    "            ['X_test', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "             split_method]) + '.txt')\n",
    "    \n",
    "    y_train_filename = \"_\".join(\n",
    "        ['y_train', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "         split_method]) + '.txt'\n",
    "\n",
    "    y_test_filename = \"_\".join(\n",
    "        ['y_test', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "         split_method]) + '.txt'\n",
    "\n",
    "    X_train = load_group(MODEL_FOLDER, X_train_filenames)\n",
    "    y_train = load_file(MODEL_FOLDER, y_train_filename)\n",
    "\n",
    "    X_test = load_group(MODEL_FOLDER, X_test_filenames)\n",
    "    y_test = load_file(MODEL_FOLDER, y_test_filename)\n",
    "\n",
    "    activity_to_num_mapping = {\n",
    "        \"rest\": 0,\n",
    "        # indoor\n",
    "        \"tread_flat_walk\": 1,\n",
    "        \"tread_flat_run\": 2,\n",
    "        \"tread_slope_walk\": 3,\n",
    "        \"indoor_flat_walk\": 4,\n",
    "        \"indoor_flat_run\": 5,\n",
    "        # outdoor\n",
    "        \"outdoor_walk\": 6,\n",
    "        \"outdoor_run\": 7\n",
    "    }\n",
    "\n",
    "    y_train = np.vectorize(activity_to_num_mapping.get)(y_train)\n",
    "    y_test = np.vectorize(activity_to_num_mapping.get)(y_test)\n",
    "\n",
    "    # convert to binary class matrix\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T16:04:55.279552Z",
     "start_time": "2019-10-29T16:04:55.271107Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "aUoBgDSPQXAb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create LSTM Model\n",
    "def createLSTMModel(n_timesteps, n_features, n_outputs):\n",
    "    ipt = Input(shape=(n_timesteps, n_features))\n",
    "    x = LSTM(100)(ipt)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(n_outputs, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=ipt, outputs=x)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def createConv1DModel(n_timesteps, n_features, n_outputs):\n",
    "    ipt = Input(shape=(n_timesteps, n_features))\n",
    "    x = Conv1D(128, 3, activation='relu')(ipt)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = MaxPooling1D(4)(x)\n",
    "    x = Conv1D(64, 3, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = MaxPooling1D(4)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(28, activation='relu')(x)\n",
    "    x = Dense(n_outputs, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=ipt, outputs=x)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T16:05:07.326093Z",
     "start_time": "2019-10-29T16:05:07.313629Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "o1vDN3nCQXAd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_predict_plot(X_train, y_train, X_test, y_test, model_name='v1', verbose=1, epochs=5, batch_size=128, model_type=\"LSTM\"):\n",
    "    if model_type == \"LSTM\":\n",
    "        model = createLSTMModel(n_timesteps=X_train.shape[1], n_features=X_train.shape[2], n_outputs=y_train.shape[1])\n",
    "    else:\n",
    "        model = createConv1DModel(n_timesteps=X_train.shape[1], n_features=X_train.shape[2], n_outputs=y_train.shape[1])\n",
    "    model.summary()\n",
    "\n",
    "    filepath = os.path.join(model_name + \".hdf5\")\n",
    "    checkpoint = ModelCheckpoint(filepath,\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True,\n",
    "                                 mode='max')\n",
    "\n",
    "    # Log the epoch detail into csv\n",
    "    csv_logger = CSVLogger(os.path.join(model_name + '.csv'))\n",
    "    callbacks_list = [checkpoint, csv_logger]\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=epochs, batch_size=batch_size,\n",
    "                        verbose=verbose, callbacks=callbacks_list)\n",
    "    \n",
    "    model.load_weights(filepath) # load best model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # plot model accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(model_name + '_model_acc', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.savefig(model_name + '_model_loss', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    class_label = np.concatenate((const_indoor_activity, const_outdoor_activity))\n",
    "\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"Best accuracy (on validation dataset): %.2f%%\" % (accuracy_score(y_test, y_pred) * 100))\n",
    "    print(cnf_matrix)\n",
    "\n",
    "    plt.figure()\n",
    "    fig, ax = plot_confusion_matrix(conf_mat=cnf_matrix,\n",
    "                                    show_absolute=True,\n",
    "                                    show_normed=True,\n",
    "                                    colorbar=True,\n",
    "                                    class_names=class_label,\n",
    "                                    figsize=(10, 10))\n",
    "    plt.savefig(model_name + '_confusion_matrix', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # display report\n",
    "    report_display = classification_report(y_test, y_pred, target_names=class_label, digits=4)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report_display)\n",
    "\n",
    "    # create report and store in csv\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    df_report.to_csv(model_name + '_classification_report')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To select the BEST hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T15:48:50.176029Z",
     "start_time": "2019-10-29T15:48:50.172962Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SNGeYCGGQXAe",
    "outputId": "962857f9-f079-439c-f48d-5d0715771b70",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# axis_list = ['accX', 'accY', 'accZ']\n",
    "# pos_list = ['LF', 'RF', 'Waist', 'Wrist']\n",
    "\n",
    "seed = 29\n",
    "np.random.seed(seed)\n",
    "\n",
    "window_list = [512,\n",
    "               256\n",
    "               ]\n",
    "\n",
    "wavelet_args_list = [\n",
    "    {\n",
    "        \"type\": \"Y\",\n",
    "        \"threshold\": 2,\n",
    "        \"wavedec_options\": {\"wavelet\": \"db4\", \"level\": 2},\n",
    "        \"waverec_options\": {\"wavelet\": \"db4\"}\n",
    "    },\n",
    "    {\"type\": \"N\"}\n",
    "]\n",
    "\n",
    "window_args_list = [#{\"type\": \"by_peaks\", \"find_peaks_options\": {\"prominence\": 30, \"height\": 20}}, \n",
    "                    {\"type\":\"with_overlap\", \"overlap_perc\":0.5}, \n",
    "                    {\"type\":\"no_overlap\"}\n",
    "]\n",
    "\n",
    "split_method_list = ['TrainTestSplitWithinSubject',\n",
    "                    'Random',\n",
    "                    'DifferentSubjectsInTrainTest'\n",
    "                     ]\n",
    "\n",
    "model_type=\"LSTM\"\n",
    "\n",
    "feature_list_list = [\n",
    "    ['accX_LF', 'accY_LF'\n",
    "      , 'accZ_LF', 'accX_RF', 'accY_RF', 'accZ_RF',\n",
    "      'accX_Waist', 'accY_Waist', 'accZ_Waist', 'accX_Wrist', 'accY_Wrist', 'accZ_Wrist']\n",
    "]\n",
    "\n",
    "count = 0\n",
    "for split_method in split_method_list:\n",
    "    for window in window_list:\n",
    "        for wavelet_args in wavelet_args_list:\n",
    "            for window_args in window_args_list:\n",
    "                for feature_list in feature_list_list:\n",
    "                    count = count + 1\n",
    "                    model_name = \"_\".join([\"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"], split_method])\n",
    "                    \n",
    "                    feature_col = feature_list[0]\n",
    "                    \n",
    "                    for feature in feature_list:\n",
    "                        prepare_single_feature_to_csv(feature, feature_col, model_name, split_method, window,\n",
    "                                                      wavelet_args, window_args)\n",
    "\n",
    "                    X_train, y_train, X_test, y_test = load_dataset(model_name, feature_list, window, wavelet_args, window_args,\n",
    "                                                                    split_method)\n",
    "\n",
    "                    print(X_train.shape)\n",
    "                    print(y_train.shape)\n",
    "                    print(X_test.shape)\n",
    "                    print(y_test.shape)\n",
    "                    print(X_train.shape[1])\n",
    "                    print(X_train.shape[2])\n",
    "                    print(y_train.shape[1])\n",
    "\n",
    "                    np.random.seed(seed)\n",
    "                    train_predict_plot(X_train, y_train, X_test, y_test, model_name=model_name, verbose=1, epochs=20,\n",
    "                                       batch_size=300, model_type=model_type)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To select the best sensor combinations, with the given BEST hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T16:13:48.484583Z",
     "start_time": "2019-10-29T16:05:14.195523Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# axis_list = ['accX', 'accY', 'accZ']\n",
    "# pos_list = ['LF', 'RF', 'Waist', 'Wrist']\n",
    "\n",
    "seed = 29\n",
    "np.random.seed(seed)\n",
    "\n",
    "window = 256\n",
    "\n",
    "wavelet_args = {\"type\": \"Y\",\n",
    "                \"threshold\": 2,\n",
    "                \"wavedec_options\": {\"wavelet\": \"db4\", \"level\": 2},\n",
    "                \"waverec_options\": {\"wavelet\": \"db4\"}}\n",
    "\n",
    "window_args = {\"type\":\"with_overlap\", \"overlap_perc\":0.5}\n",
    "\n",
    "split_method = 'Random'\n",
    "\n",
    "# the folder that keeps the extracted data with the best hyper-parameters \n",
    "model_name = \"_\".join([\"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"], split_method])\n",
    "\n",
    "feature_list_list = [\n",
    "    ['accX_LF', 'accY_LF', 'accZ_LF'],\n",
    "    ['accX_RF', 'accY_RF', 'accZ_RF'],\n",
    "    ['accX_Waist', 'accY_Waist', 'accZ_Waist'],\n",
    "    ['accX_Wrist', 'accY_Wrist', 'accZ_Wrist'],\n",
    "]\n",
    "\n",
    "# feature_list_list = [\n",
    "#     ['accX_LF', 'accY_LF', 'accZ_LF', 'accX_RF', 'accY_RF', 'accZ_RF'],\n",
    "#     ['accX_LF', 'accY_LF', 'accZ_LF', 'accX_Waist', 'accY_Waist', 'accZ_Waist'],\n",
    "#     ['accX_LF', 'accY_LF', 'accZ_LF', 'accX_Wrist', 'accY_Wrist', 'accZ_Wrist']\n",
    "# ]\n",
    "\n",
    "# feature_list_list = [\n",
    "#     ['accX_LF', 'accY_LF'],\n",
    "#     ['accX_LF', 'accZ_LF'],\n",
    "#     ['accY_LF', 'accZ_LF'],\n",
    "#     ['accX_LF'],\n",
    "#     ['accY_LF'],\n",
    "#     ['accZ_LF']\n",
    "# ]\n",
    "\n",
    "model_type=\"LSTM\"\n",
    "\n",
    "for feature_list in feature_list_list:\n",
    "    X_train, y_train, X_test, y_test = load_dataset(model_name, feature_list, window, wavelet_args, window_args,\n",
    "                                                    split_method)\n",
    "\n",
    "    print(\"Trying for these features %s\" %(feature_list))\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    print(X_train.shape[1])\n",
    "    print(X_train.shape[2])\n",
    "    print(y_train.shape[1])\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    train_predict_plot(X_train, y_train, X_test, y_test, model_name=model_name + \"-\".join(feature_list), verbose=1, epochs=20,\n",
    "                       batch_size=300, model_type=model_type)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To load outputs of a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters ###\n",
    "\n",
    "window = 256\n",
    "\n",
    "wavelet_args = {\"type\": \"Y\",\n",
    "                \"threshold\": 2,\n",
    "                \"wavedec_options\": {\"wavelet\": \"db4\", \"level\": 2},\n",
    "                \"waverec_options\": {\"wavelet\": \"db4\"}}\n",
    "\n",
    "window_args = {\"type\":\"with_overlap\", \"overlap_perc\":0.5}\n",
    "\n",
    "split_method = 'Random'\n",
    "\n",
    "# the folder that keeps the extracted data with the best hyper-parameters \n",
    "model_name = \"_\".join([\"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"], split_method])\n",
    "\n",
    "feature_list = ['accX_LF', 'accY_LF', 'accZ_LF']\n",
    "\n",
    "model_type=\"LSTM\"\n",
    "\n",
    "### Parameters ###\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_dataset(model_name, feature_list, window, wavelet_args, window_args, split_method)\n",
    "\n",
    "if model_type == \"LSTM\":\n",
    "    model = createLSTMModel(n_timesteps=X_train.shape[1], n_features=X_train.shape[2], n_outputs=y_train.shape[1])\n",
    "else:\n",
    "    model = createConv1DModel(n_timesteps=X_train.shape[1], n_features=X_train.shape[2], n_outputs=y_train.shape[1])\n",
    "model.summary()\n",
    "\n",
    "filepath = model_name + \"-\".join(feature_list) + \".hdf5\"\n",
    "\n",
    "model.load_weights(filepath) # load best model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "class_label = np.concatenate((const_indoor_activity, const_outdoor_activity))\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Best accuracy (on validation dataset): %.2f%%\" % (accuracy_score(y_test, y_pred) * 100))\n",
    "print(cnf_matrix)\n",
    "\n",
    "plt.figure()\n",
    "fig, ax = plot_confusion_matrix(conf_mat=cnf_matrix,\n",
    "                                show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True,\n",
    "                                class_names=class_label,\n",
    "                                figsize=(10, 10))\n",
    "plt.savefig(model_name + '_confusion_matrix', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# display report\n",
    "report_display = classification_report(y_test, y_pred, target_names=class_label, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report_display)\n",
    "\n",
    "# create report and store in csv\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report.to_csv(model_name + '_classification_report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CA3_SuperLoop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
